diff --git a/.vscode/launch.json b/.vscode/launch.json
index 06dfc45..11813ca 100644
--- a/.vscode/launch.json
+++ b/.vscode/launch.json
@@ -8,10 +8,9 @@
       "type": "node",
       "request": "launch",
       "name": "Slackbot: Debug",
-      "cwd": "${workspaceFolder}/services/slackbot",
-      "program": "${workspaceFolder}/node_modules/ts-node/dist/bin.js",
-      "args": ["${workspaceFolder}/services/slackbot/src/app.ts"],
-      "envFile": "${workspaceFolder}/services/slackbot/.env"
+      "cwd": "${workspaceFolder}",
+      "program": "${workspaceFolder}/node_modules/.bin/nx",
+      "args": ["dev", "slackbot"]
     },
     {
       "type": "node",
@@ -32,7 +31,10 @@
       "args": ["dev", "api"],
       "sourceMaps": true,
       "env": {
-        "TELEMETRY_ENABLED": "false"
+        "TELEMETRY_ENABLED": "false",
+        "LANGFUSE_SECRET_KEY": "sk-lf-17e73530-d170-4e27-af09-ed2d5c880b5d",
+        "LANGFUSE_PUBLIC_KEY": "pk-lf-fa24bd19-d299-4dc5-bd63-3b32237e1bec",
+        "LANGFUSE_HOST": "https://us.cloud.langfuse.com"
       },
       "cwd": "${workspaceFolder}",
       "internalConsoleOptions": "neverOpen"
diff --git a/TROUBLESHOOTING.md b/TROUBLESHOOTING.md
index 24db13e..a22e832 100644
--- a/TROUBLESHOOTING.md
+++ b/TROUBLESHOOTING.md
@@ -26,6 +26,10 @@ This error usually happens when the Slack keys (`SLACK_BOT_TOKEN`, `SLACK_APP_TO
 
 If they are correct, try to restart the `slackbot` service by running `docker compose up slackbot -d`. Sometimes users update `.env` but do not restart the service itself, which causing it to take out-dated variables.
 
+### `429: Too Many Requests`
+
+This error might appear in LiteLLM's logs. It's a bit misleading, and most of the times it means you don't have enough credits left. Go to your LLM provider (OpenAI, Anthropic, etc.) and check your credits.
+
 ### Environment variabels are out-dated
 
 If you use VSC Code, sometimes it loads environment variables from the `.env` file automatically. In most cases, it happens because of the python extension. In our `settings.json`, we set `"python.envFile": ""` which shoud prevent that. However, if that doesn't work, try to run the project from a separate terminal (not VS Code).
diff --git a/services/api/src/agent/tools/static/semantic_search.ts b/services/api/src/agent/tools/static/semantic_search.ts
index 32a2a9d..8dc8e98 100644
--- a/services/api/src/agent/tools/static/semantic_search.ts
+++ b/services/api/src/agent/tools/static/semantic_search.ts
@@ -88,6 +88,7 @@ export default async function (context: RunContext) {
                   title = "PagerDuty Alert";
                   break;
                 }
+                case "Jira":
                 case "Confluence": {
                   url = document.metadata.url;
                   title = document.metadata.title;
diff --git a/services/data-processor/src/build.py b/services/data-processor/src/build.py
index 2a249b3..a664d87 100644
--- a/services/data-processor/src/build.py
+++ b/services/data-processor/src/build.py
@@ -42,14 +42,8 @@ async def build_index(
 
         store = get_vector_store(index_name, index_type)
 
-        try:
-            if await store.is_index_live():
-                print("Index exists. Delete old one...")
-                await store.delete_index()
-        except Exception as e:
-            print("Could not delete index", e)
-            print("Trying to move forward")
-        await store.create_index()
+        if not await store.is_index_live():
+            await store.create_index()
 
         async def update_status(vendor_name: str, status: str):
             await db.index.update_one(
@@ -57,14 +51,20 @@ async def build_index(
                 {"$set": {f"state.integrations.{vendor_name}": status}},
             )
 
+        vector_store = store.get_llama_index_store()
         documents, stats = await get_documents(
+            index=index,
+            vector_store=vector_store,
             organization_id=organization_id,
             data_sources=data_sources,
             on_progress=partial(update_status, status="in_progress"),
             on_complete=partial(update_status, status="completed"),
         )
+        # Delete nodes of documents that are about to be re-indexed
+        if len(documents) > 0:
+            docs_to_delete = list(set([document.ref_doc_id for document in documents]))
+            vector_store.delete(ref_doc_id=docs_to_delete)
 
-        vector_store = store.get_llama_index_store()
         storage_context = StorageContext.from_defaults(vector_store=vector_store)
         embed_model = LiteLLMEmbedding(
             api_base=litellm_url,
diff --git a/services/data-processor/src/loader.py b/services/data-processor/src/loader.py
index 903b5f4..aa15c16 100644
--- a/services/data-processor/src/loader.py
+++ b/services/data-processor/src/loader.py
@@ -1,6 +1,7 @@
 import os
 import asyncio
-from typing import List, Optional
+from typing import List, Optional, Any
+from dateutil import parser
 import numpy as np
 from tqdm.auto import tqdm
 from db.integrations import get_integrations_by_organization_id, populate_secrets
@@ -11,11 +12,86 @@ from llama_index.core.settings import (
     Settings,
 )
 
+from llama_index.core.schema import Document
+from llama_index.core.vector_stores.types import (
+    BasePydanticVectorStore,
+    VectorStoreQuery,
+    MetadataFilters,
+    MetadataFilter,
+    FilterOperator,
+)
+
+
+async def filter_unchanged_documents(
+    vector_store: BasePydanticVectorStore,
+    documents: List[Document],
+):
+    # Create a dictionary to group documents by ref_doc_id
+    document_ids = [document.doc_id for document in documents]
+
+    result = vector_store.query(
+        VectorStoreQuery(
+            similarity_top_k=100000000000,  # TODO: This is a hack to make sure we get all the documents
+            filters=MetadataFilters(
+                filters=[
+                    MetadataFilter(
+                        key="ref_doc_id",
+                        value=document_ids,
+                        operator=FilterOperator.IN,
+                    )
+                ]
+            ),
+        )
+    )
+    db_nodes = result.nodes
+    if len(db_nodes) == 0:
+        return [], [], documents
+
+    db_nodes_groups = {}
+    for db_node in db_nodes:
+        ref_doc_id = db_node.ref_doc_id
+        if ref_doc_id not in db_nodes_groups:
+            db_nodes_groups[ref_doc_id] = []
+        db_nodes_groups[ref_doc_id].append(db_node)
+
+    new_documents = []
+    unchanged_documents = []
+    changed_documents = []
+
+    for document in documents:
+        # At the moment, if the document doesn't have an updated_at, we re-index it
+        if not document.metadata.get("updated_at"):
+            changed_documents.append(document)
+            continue
+
+        document_id = document.doc_id
+        document_nodes = db_nodes_groups.get(document_id, [])
+        if len(document_nodes) > 0:
+            document_timestamp = parser.isoparse(document.metadata["updated_at"])
+            node_timestamp = parser.isoparse(document_nodes[0].metadata["updated_at"])
+
+            # If the document's updated_at date is greater than the node's updated_at date, it means
+            # the document has been updated, so we need to re-index it
+            if document_timestamp > node_timestamp:
+                changed_documents.append(document)
+            else:
+                unchanged_documents.append(document)
+        else:
+            new_documents.append(document)
+
+    print(f"Found {len(changed_documents)} changed documents")
+    print(f"Found {len(unchanged_documents)} unchanged documents")
+    print(f"Found {len(new_documents)} new documents")
+
+    return changed_documents, unchanged_documents, new_documents
+
 
 async def get_documents(
+    index: Any,
+    vector_store: BasePydanticVectorStore,
     organization_id: str,
     data_sources: Optional[List[str]] = None,
-    total_limit: Optional[int] = 10000,
+    total_limit: Optional[int] = 10000,  # unused at the moment
     on_progress: Optional[callable] = None,
     on_complete: Optional[callable] = None,
 ):
@@ -30,14 +106,11 @@ async def get_documents(
     vendor_names = [integration.vendor.name for integration in integrations]
     print(f"Found {len(integrations)} integrations: {vendor_names}")
 
-    # Calculate the limit per source
-    limit_per_source = round(total_limit / len(integrations))
-
     stats = {}
-    documents = []
-
-    # Settings.transformations
+    total_nodes = []
     progress_bar = tqdm(integrations)
+    n_existing_nodes = index.get("stats") and sum(index["stats"].values()) or 0
+
     for integration in progress_bar:
         vendor_name = integration.vendor.name
         if on_progress:
@@ -52,9 +125,13 @@ async def get_documents(
         # Loader might be an async code, so we need to await it
         try:
             if asyncio.iscoroutinefunction(loader):
-                docs = await loader(integration)
+                raw_docs = await loader(integration)
             else:
-                docs = loader(integration)
+                raw_docs = loader(integration)
+
+            changed_documents, unchanged_documents, new_documents = (
+                await filter_unchanged_documents(vector_store, raw_docs)
+            )
         except Exception as e:
             print(f"Could not load {vendor_name}. Error: {e}")
             continue
@@ -66,19 +143,17 @@ async def get_documents(
         num_cpus = os.cpu_count()
         num_workers = min(4, num_cpus) if num_cpus > 1 else 1
 
-        # counts = [len(doc.text) for doc in docs]
-        # limit = np.percentile(counts, [99])[0]
-        # docs = [doc for doc in docs if len(doc.text) < limit]
-        docs = pipeline.run(documents=docs, num_workers=num_workers)
-
-        # Limit the number of documents per source
-        # docs = docs[:limit_per_source]
+        new_nodes = pipeline.run(documents=new_documents, num_workers=num_workers)
+        changed_nodes = pipeline.run(
+            documents=changed_documents, num_workers=num_workers
+        )
+        nodes = new_nodes + changed_nodes
 
-        print(f"Found {len(docs)} documents for {vendor_name}")
-        documents.extend(docs)
-        stats[integration.vendor.name] = len(docs)
+        print(f"Found total of {len(raw_docs)} documents for {vendor_name}")
+        total_nodes.extend(nodes)
+        stats[integration.vendor.name] = n_existing_nodes + len(new_nodes)
 
         if on_complete:
             await on_complete(vendor_name)
 
-    return documents, stats
+    return total_nodes, stats
diff --git a/services/data-processor/src/loaders/confluence.py b/services/data-processor/src/loaders/confluence.py
index dba6ebc..5bcad47 100644
--- a/services/data-processor/src/loaders/confluence.py
+++ b/services/data-processor/src/loaders/confluence.py
@@ -1,7 +1,7 @@
 from collections import namedtuple
 import os
 import requests
-from loaders.raw_readers.confluence import ConfluenceReader
+from loaders.readers.confluence import ConfluenceReader
 from atlassian import Confluence
 
 from db.types import Integration
diff --git a/services/data-processor/src/loaders/github.py b/services/data-processor/src/loaders/github.py
index 14db1d3..2dab9d6 100644
--- a/services/data-processor/src/loaders/github.py
+++ b/services/data-processor/src/loaders/github.py
@@ -2,13 +2,13 @@ from tqdm.auto import tqdm
 from github import Github, Auth, GithubException
 
 # from llama_index.core import SimpleDirectoryReader
-from llama_index.readers.github.repository.github_client import GithubClient
+from loaders.utils.github_client import GithubClient
 from llama_index.readers.github import (
     GitHubIssuesClient,
 )
 from db.types import Integration
-from loaders.raw_readers.github_repo import GithubRepositoryReader
-from loaders.raw_readers.github_issues import GitHubRepositoryIssuesReader
+from loaders.readers.github_repo import GithubRepositoryReader
+from loaders.readers.github_issues import GitHubRepositoryIssuesReader
 
 
 def get_repos(token: str, repos_to_sync=None):
@@ -70,6 +70,8 @@ async def fetch_github_documents(
             # # TODO: this can crash if the repo is huge, because of Github API Rate limit.
             # # Need to find a way to "wait" maybe or to filter garbage.
             code_client = GithubClient(token, fail_on_http_error=False, verbose=True)
+
+            # TODO: updated_at timestamp doesn't seem to work (our code treats same docs as new)
             loader = GithubRepositoryReader(
                 github_client=code_client,
                 owner=owner,
diff --git a/services/data-processor/src/loaders/jira.py b/services/data-processor/src/loaders/jira.py
index c7adaf3..a4eb984 100644
--- a/services/data-processor/src/loaders/jira.py
+++ b/services/data-processor/src/loaders/jira.py
@@ -1,7 +1,11 @@
 import requests
-from llama_index.readers.jira import JiraReader
+from datetime import datetime, timezone
+from dateutil import parser
+from loaders.readers.jira import JiraReader
 from db.types import Integration
 
+JQL_QUERY = "issuetype is not EMPTY"
+
 
 def fetch_jira_documents(integration: Integration):
     integration_type = integration.type
@@ -19,9 +23,7 @@ def fetch_jira_documents(integration: Integration):
             loader = JiraReader(
                 Oauth2={"cloud_id": cloud_id, "api_token": access_token}
             )
-            documents = loader.load_data(
-                "issuetype is not EMPTY"
-            )  # This "should" fetch all issues
+            documents = loader.load_data(JQL_QUERY)  # This "should" fetch all issues
             total_documents.extend(documents)
     else:
         loader = JiraReader(
@@ -31,7 +33,7 @@ def fetch_jira_documents(integration: Integration):
                 "server_url": integration.metadata["site_url"],
             }
         )
-        documents = loader.load_data("issuetype is not EMPTY")
+        documents = loader.load_data(JQL_QUERY)
         total_documents.extend(documents)
 
     # Adding the global "source" metadata field
@@ -39,4 +41,16 @@ def fetch_jira_documents(integration: Integration):
         document.metadata.pop("labels", None)
         document.metadata["source"] = "Jira"
 
-    return documents
+        # Transform 'created_at' and 'updated_at' to UTC with milliseconds
+        created_at = parser.isoparse(document.metadata["created_at"])
+        updated_at = parser.isoparse(document.metadata["updated_at"])
+        document.metadata["created_at"] = (
+            created_at.astimezone(timezone.utc).strftime("%Y-%m-%dT%H:%M:%S.%f")[:-3]
+            + "Z"
+        )
+        document.metadata["updated_at"] = (
+            updated_at.astimezone(timezone.utc).strftime("%Y-%m-%dT%H:%M:%S.%f")[:-3]
+            + "Z"
+        )
+
+    return total_documents
diff --git a/services/data-processor/src/loaders/notion.py b/services/data-processor/src/loaders/notion.py
index 267d23d..0a0338c 100644
--- a/services/data-processor/src/loaders/notion.py
+++ b/services/data-processor/src/loaders/notion.py
@@ -1,6 +1,6 @@
 from db.types import Integration
 from notion_client import Client
-from llama_index.readers.notion import NotionPageReader
+from loaders.readers.notion import NotionPageReader
 
 
 def fetch_notion_documents(integration: Integration):
diff --git a/services/data-processor/src/loaders/pagerduty.py b/services/data-processor/src/loaders/pagerduty.py
index 7899c4a..c8b3c99 100644
--- a/services/data-processor/src/loaders/pagerduty.py
+++ b/services/data-processor/src/loaders/pagerduty.py
@@ -1,82 +1,11 @@
 from db.types import Integration
-import httpx
-from llama_index.core import Document
-
-INCIDENT_TEXT_TEMPLATE = """
-Incident title: {title}
-Incident description: {description}
-Incident summary: {summary}
-Incident status: {status}
-Service name: {service_name}
-Created at: {created_at}
-"""
-
-
-async def get_incidents(integration: Integration):
-    access_token = integration.credentials["access_token"]
-    integration_type = integration.type
-    headers = {}
-    if integration_type == "basic":
-        headers["Authorization"] = f"Token token={access_token}"
-    elif integration_type == "oauth":
-        headers["Authorization"] = f"Bearer {access_token}"
-    else:
-        raise ValueError(f"Invalid integration type: {integration_type}")
-
-    limit = 100
-    offset = 0
-    resolved_incidents = []
-    while True:
-        async with httpx.AsyncClient() as client:
-            response = await client.get(
-                "https://api.pagerduty.com/incidents",
-                headers=headers,
-                params={
-                    "date_range": "all",
-                    "statuses[]": "resolved",
-                    "limit": limit,
-                    "offset": offset,
-                },
-            )
-            data = response.json()
-            incidents = data["incidents"]
-            resolved_incidents.extend(incidents)
-            if not data["more"]:
-                break
-            offset += limit
-    return resolved_incidents
+from loaders.readers.pagerduty import PagerDutyReader
 
 
 async def fetch_pagerduty_documents(integration: Integration):
-    incidents = await get_incidents(integration)
-
-    documents = []
-    for incident in incidents:
-        service = incident.get("service", {})
-        service_name = service.get("summary", "Unknown")
-
-        text = INCIDENT_TEXT_TEMPLATE.format(
-            title=incident["title"],
-            description=incident["description"],
-            summary=incident["summary"],
-            status=incident["status"],
-            service_name=service_name,
-            created_at=incident["created_at"],
-        )
-        metadata = {
-            "source": "PagerDuty",
-            "id": incident["id"],
-            "link": incident["html_url"],
-            "status": incident["status"],
-            "urgency": incident["urgency"],
-            "service_id": service.get("id", "Unknown"),
-            "first_trigger_log_entry_id": incident.get(
-                "first_trigger_log_entry", {}
-            ).get("id", "Unknown"),
-            "created_at": incident["created_at"],
-        }
-
-        document = Document(doc_id=incident["id"], text=text, metadata=metadata)
-        documents.append(document)
+    access_token = integration.credentials["access_token"]
+    token_type = integration.type
+    loader = PagerDutyReader(access_token, token_type)
+    documents = await loader.load_data()
 
     return documents
diff --git a/services/data-processor/src/loaders/raw_readers/README.md b/services/data-processor/src/loaders/readers/README.md
similarity index 100%
rename from services/data-processor/src/loaders/raw_readers/README.md
rename to services/data-processor/src/loaders/readers/README.md
diff --git a/services/data-processor/src/loaders/raw_readers/confluence.py b/services/data-processor/src/loaders/readers/confluence.py
similarity index 98%
rename from services/data-processor/src/loaders/raw_readers/confluence.py
rename to services/data-processor/src/loaders/readers/confluence.py
index 5823e06..8a4c8f1 100644
--- a/services/data-processor/src/loaders/raw_readers/confluence.py
+++ b/services/data-processor/src/loaders/readers/confluence.py
@@ -163,6 +163,7 @@ class ConfluenceReader(BaseReader):
         if not start:
             start = 0
 
+        expand = "body.export_view.value,version"
         pages: List = []
         if space_key:
             pages.extend(
@@ -172,7 +173,7 @@ class ConfluenceReader(BaseReader):
                     max_num_results=max_num_results,
                     space=space_key,
                     status=page_status,
-                    expand="body.export_view.value",
+                    expand=expand,
                     content_type="page",
                 )
             )
@@ -183,7 +184,7 @@ class ConfluenceReader(BaseReader):
                     cursor=cursor,
                     cql=f'type="page" AND label="{label}"',
                     max_num_results=max_num_results,
-                    expand="body.export_view.value",
+                    expand=expand,
                 )
             )
         elif cql:
@@ -193,7 +194,7 @@ class ConfluenceReader(BaseReader):
                     cursor=cursor,
                     cql=cql,
                     max_num_results=max_num_results,
-                    expand="body.export_view.value",
+                    expand=expand,
                 )
             )
         elif page_ids:
@@ -217,7 +218,7 @@ class ConfluenceReader(BaseReader):
                     self._get_data_with_retry(
                         self.confluence.get_page_by_id,
                         page_id=page_id,
-                        expand="body.export_view.value",
+                        expand=expand,
                     )
                 )
 
@@ -342,6 +343,7 @@ class ConfluenceReader(BaseReader):
                 "page_id": page["id"],
                 "status": page["status"],
                 "url": self.base_url + page["_links"]["webui"],
+                "updated_at": page["version"]["when"],
             },
         )
 
diff --git a/services/data-processor/src/loaders/raw_readers/github_issues.py b/services/data-processor/src/loaders/readers/github_issues.py
similarity index 99%
rename from services/data-processor/src/loaders/raw_readers/github_issues.py
rename to services/data-processor/src/loaders/readers/github_issues.py
index b5f64e4..8b61d57 100644
--- a/services/data-processor/src/loaders/raw_readers/github_issues.py
+++ b/services/data-processor/src/loaders/readers/github_issues.py
@@ -183,6 +183,7 @@ class GitHubRepositoryIssuesReader(BaseReader):
                 extra_info = {
                     "state": issue["state"],
                     "created_at": issue["created_at"],
+                    "updated_at": issue["updated_at"],
                     # url is the API URL
                     "url": issue["url"],
                     # source is the HTML URL, more convenient for humans
diff --git a/services/data-processor/src/loaders/raw_readers/github_repo.py b/services/data-processor/src/loaders/readers/github_repo.py
similarity index 98%
rename from services/data-processor/src/loaders/raw_readers/github_repo.py
rename to services/data-processor/src/loaders/readers/github_repo.py
index 5ac7590..576f210 100644
--- a/services/data-processor/src/loaders/raw_readers/github_repo.py
+++ b/services/data-processor/src/loaders/readers/github_repo.py
@@ -446,6 +446,10 @@ class GithubRepositoryReader(BaseReader):
             )
         return blobs_and_full_paths
 
+    async def _get_latest_commit(self, path) -> str:
+        commits = await self._github_client.get_commits(self._owner, self._repo, path)
+        return commits[0]
+
     async def _generate_documents(
         self,
         blobs_and_paths: List[Tuple[GitTreeResponseModel.GitTreeObject, str]],
@@ -472,6 +476,7 @@ class GithubRepositoryReader(BaseReader):
         documents = []
         async for blob_data, full_path in buffered_iterator:
             print_if_verbose(self._verbose, f"generating document for {full_path}")
+            latest_commit = await self._get_latest_commit(full_path)
             assert (
                 blob_data.encoding == "base64"
             ), f"blob encoding {blob_data.encoding} not supported"
@@ -525,6 +530,7 @@ class GithubRepositoryReader(BaseReader):
                     "file_path": full_path,
                     "file_name": full_path.split("/")[-1],
                     "url": url,
+                    "updated_at": latest_commit.commit.author.date,
                 },
             )
             documents.append(document)
diff --git a/services/data-processor/src/loaders/readers/jira.py b/services/data-processor/src/loaders/readers/jira.py
new file mode 100644
index 0000000..75bf63a
--- /dev/null
+++ b/services/data-processor/src/loaders/readers/jira.py
@@ -0,0 +1,117 @@
+from typing import List, Optional, TypedDict
+
+from llama_index.core.readers.base import BaseReader
+from llama_index.core.schema import Document
+
+
+class BasicAuth(TypedDict):
+    email: str
+    api_token: str
+    server_url: str
+
+
+class Oauth2(TypedDict):
+    cloud_id: str
+    api_token: str
+
+
+class JiraReader(BaseReader):
+    """Jira reader. Reads data from Jira issues from passed query.
+
+    Args:
+        Optional basic_auth:{
+            "email": "email",
+            "api_token": "token",
+            "server_url": "server_url"
+        }
+        Optional oauth:{
+            "cloud_id": "cloud_id",
+            "api_token": "token"
+        }
+    """
+
+    def __init__(
+        self,
+        email: Optional[str] = None,
+        api_token: Optional[str] = None,
+        server_url: Optional[str] = None,
+        BasicAuth: Optional[BasicAuth] = None,
+        Oauth2: Optional[Oauth2] = None,
+    ) -> None:
+        from jira import JIRA
+
+        if email and api_token and server_url:
+            if BasicAuth is None:
+                BasicAuth = {}
+            BasicAuth["email"] = email
+            BasicAuth["api_token"] = api_token
+            BasicAuth["server_url"] = server_url
+
+        if Oauth2:
+            options = {
+                "server": f"https://api.atlassian.com/ex/jira/{Oauth2['cloud_id']}",
+                "headers": {"Authorization": f"Bearer {Oauth2['api_token']}"},
+            }
+            self.jira = JIRA(options=options)
+        else:
+            self.jira = JIRA(
+                basic_auth=(BasicAuth["email"], BasicAuth["api_token"]),
+                server=f"https://{BasicAuth['server_url']}",
+            )
+
+    def load_data(self, query: str) -> List[Document]:
+        relevant_issues = self.jira.search_issues(query)
+
+        issues = []
+
+        assignee = ""
+        reporter = ""
+        epic_key = ""
+        epic_summary = ""
+        epic_descripton = ""
+
+        for issue in relevant_issues:
+            # Iterates through only issues and not epics
+            if "parent" in (issue.raw["fields"]):
+                if issue.fields.assignee:
+                    assignee = issue.fields.assignee.displayName
+
+                if issue.fields.reporter:
+                    reporter = issue.fields.reporter.displayName
+
+                if issue.raw["fields"]["parent"]["key"]:
+                    epic_key = issue.raw["fields"]["parent"]["key"]
+
+                if issue.raw["fields"]["parent"]["fields"]["summary"]:
+                    epic_summary = issue.raw["fields"]["parent"]["fields"]["summary"]
+
+                if issue.raw["fields"]["parent"]["fields"]["status"]["description"]:
+                    epic_descripton = issue.raw["fields"]["parent"]["fields"]["status"][
+                        "description"
+                    ]
+
+            issues.append(
+                Document(
+                    text=f"{issue.fields.summary} \n {issue.fields.description}",
+                    doc_id=issue.id,
+                    extra_info={
+                        "id": issue.id,
+                        "title": issue.fields.summary,
+                        "url": issue.permalink(),
+                        "created_at": issue.fields.created,
+                        "updated_at": issue.fields.updated,
+                        "labels": issue.fields.labels,
+                        "status": issue.fields.status.name,
+                        "assignee": assignee,
+                        "reporter": reporter,
+                        "project": issue.fields.project.name,
+                        "issue_type": issue.fields.issuetype.name,
+                        "priority": issue.fields.priority.name,
+                        "epic_key": epic_key,
+                        "epic_summary": epic_summary,
+                        "epic_description": epic_descripton,
+                    },
+                )
+            )
+
+        return issues
diff --git a/services/data-processor/src/loaders/readers/notion.py b/services/data-processor/src/loaders/readers/notion.py
new file mode 100644
index 0000000..1b722d7
--- /dev/null
+++ b/services/data-processor/src/loaders/readers/notion.py
@@ -0,0 +1,211 @@
+"""Notion reader."""
+
+from datetime import datetime
+import os
+from typing import Any, Dict, List, Optional
+
+import requests  # type: ignore
+from llama_index.core.readers.base import BasePydanticReader
+from llama_index.core.schema import Document
+
+INTEGRATION_TOKEN_NAME = "NOTION_INTEGRATION_TOKEN"
+BLOCK_CHILD_URL_TMPL = "https://api.notion.com/v1/blocks/{block_id}/children"
+DATABASE_URL_TMPL = "https://api.notion.com/v1/databases/{database_id}/query"
+SEARCH_URL = "https://api.notion.com/v1/search"
+
+
+def utc_to_iso(utc_time: str) -> datetime:
+    return datetime.fromisoformat(utc_time.replace("Z", "+00:00"))
+
+
+# TODO: Notion DB reader coming soon!
+class NotionPageReader(BasePydanticReader):
+    """Notion Page reader.
+
+    Reads a set of Notion pages.
+
+    Args:
+        integration_token (str): Notion integration token.
+
+    """
+
+    is_remote: bool = True
+    token: str
+    headers: Dict[str, str]
+
+    def __init__(self, integration_token: Optional[str] = None) -> None:
+        """Initialize with parameters."""
+        if integration_token is None:
+            integration_token = os.getenv(INTEGRATION_TOKEN_NAME)
+            if integration_token is None:
+                raise ValueError(
+                    "Must specify `integration_token` or set environment "
+                    "variable `NOTION_INTEGRATION_TOKEN`."
+                )
+
+        token = integration_token
+        headers = {
+            "Authorization": "Bearer " + token,
+            "Content-Type": "application/json",
+            "Notion-Version": "2022-06-28",
+        }
+
+        super().__init__(token=token, headers=headers)
+
+    @classmethod
+    def class_name(cls) -> str:
+        """Get the name identifier of the class."""
+        return "NotionPageReader"
+
+    def _read_block(self, block_id: str, num_tabs: int = 0) -> str:
+        """Read a block."""
+        done = False
+        result_lines_arr = []
+        cur_block_id = block_id
+        most_recent_time = None
+        while not done:
+            block_url = BLOCK_CHILD_URL_TMPL.format(block_id=cur_block_id)
+            query_dict: Dict[str, Any] = {}
+
+            res = requests.request(
+                "GET", block_url, headers=self.headers, json=query_dict
+            )
+            data = res.json()
+
+            for result in data["results"]:
+                result_type = result["type"]
+                result_obj = result[result_type]
+
+                cur_result_text_arr = []
+                if "rich_text" in result_obj:
+                    for rich_text in result_obj["rich_text"]:
+                        # skip if doesn't have text object
+                        if "text" in rich_text:
+                            text = rich_text["text"]["content"]
+                            prefix = "\t" * num_tabs
+                            cur_result_text_arr.append(prefix + text)
+
+                result_block_id = result["id"]
+                has_children = result["has_children"]
+                if has_children:
+                    children_text, _ = self._read_block(
+                        result_block_id, num_tabs=num_tabs + 1
+                    )
+                    cur_result_text_arr.append(children_text)
+
+                cur_result_text = "\n".join(cur_result_text_arr)
+                result_lines_arr.append(cur_result_text)
+                last_edited_time = result["last_edited_time"]
+
+                if most_recent_time is None or utc_to_iso(
+                    last_edited_time
+                ) > utc_to_iso(most_recent_time):
+                    most_recent_time = last_edited_time
+
+            if data["next_cursor"] is None:
+                done = True
+                break
+            else:
+                cur_block_id = data["next_cursor"]
+
+        block_text = "\n".join(result_lines_arr)
+
+        return block_text, most_recent_time
+
+    def read_page(self, page_id: str) -> str:
+        """Read a page."""
+        return self._read_block(page_id)
+
+    def query_database(
+        self, database_id: str, query_dict: Dict[str, Any] = {"page_size": 100}
+    ) -> List[str]:
+        """Get all the pages from a Notion database."""
+        pages = []
+
+        res = requests.post(
+            DATABASE_URL_TMPL.format(database_id=database_id),
+            headers=self.headers,
+            json=query_dict,
+        )
+        res.raise_for_status()
+        data = res.json()
+
+        pages.extend(data.get("results"))
+
+        while data.get("has_more"):
+            query_dict["start_cursor"] = data.get("next_cursor")
+            res = requests.post(
+                DATABASE_URL_TMPL.format(database_id=database_id),
+                headers=self.headers,
+                json=query_dict,
+            )
+            res.raise_for_status()
+            data = res.json()
+            pages.extend(data.get("results"))
+
+        return [page["id"] for page in pages]
+
+    def search(self, query: str) -> List[str]:
+        """Search Notion page given a text query."""
+        done = False
+        next_cursor: Optional[str] = None
+        page_ids = []
+        while not done:
+            query_dict = {
+                "query": query,
+            }
+            if next_cursor is not None:
+                query_dict["start_cursor"] = next_cursor
+            res = requests.post(SEARCH_URL, headers=self.headers, json=query_dict)
+            data = res.json()
+            for result in data["results"]:
+                page_id = result["id"]
+                page_ids.append(page_id)
+
+            if data["next_cursor"] is None:
+                done = True
+                break
+            else:
+                next_cursor = data["next_cursor"]
+        return page_ids
+
+    def load_data(
+        self, page_ids: List[str] = [], database_id: Optional[str] = None
+    ) -> List[Document]:
+        """Load data from the input directory.
+
+        Args:
+            page_ids (List[str]): List of page ids to load.
+            database_id (str): Database_id from which to load page ids.
+
+        Returns:
+            List[Document]: List of documents.
+
+        """
+        if not page_ids and not database_id:
+            raise ValueError("Must specify either `page_ids` or `database_id`.")
+        docs = []
+        if database_id is not None:
+            # get all the pages in the database
+            page_ids = self.query_database(database_id)
+            for page_id in page_ids:
+                page_text, most_recent_time = self.read_page(page_id)
+                docs.append(
+                    Document(
+                        text=page_text,
+                        id_=page_id,
+                        extra_info={"page_id": page_id, "updated_at": most_recent_time},
+                    )
+                )
+        else:
+            for page_id in page_ids:
+                page_text, most_recent_time = self.read_page(page_id)
+                docs.append(
+                    Document(
+                        text=page_text,
+                        id_=page_id,
+                        extra_info={"page_id": page_id, "updated_at": most_recent_time},
+                    )
+                )
+
+        return docs
diff --git a/services/data-processor/src/loaders/readers/pagerduty.py b/services/data-processor/src/loaders/readers/pagerduty.py
new file mode 100644
index 0000000..02e6cd4
--- /dev/null
+++ b/services/data-processor/src/loaders/readers/pagerduty.py
@@ -0,0 +1,93 @@
+import httpx
+from typing import List
+from llama_index.core.readers.base import BaseReader
+from llama_index.core.schema import Document
+
+INCIDENT_TEXT_TEMPLATE = """
+Incident title: {title}
+Incident description: {description}
+Incident summary: {summary}
+Incident status: {status}
+Service name: {service_name}
+Created at: {created_at}
+"""
+
+
+class PagerDutyReader(BaseReader):
+    access_token: str
+    token_type: str
+
+    def __init__(self, access_token: str, token_type: str):
+        self.access_token = access_token
+        self.token_type = token_type
+
+    @classmethod
+    def class_name(cls) -> str:
+        return "PagerDutyReader"
+
+    async def get_incidents(self) -> List[Document]:
+        headers = {}
+        if self.token_type == "basic":
+            headers["Authorization"] = f"Token token={self.access_token}"
+        elif self.token_type == "oauth":
+            headers["Authorization"] = f"Bearer {self.access_token}"
+
+        limit = 100
+        offset = 0
+        resolved_incidents = []
+        while True:
+            async with httpx.AsyncClient() as client:
+                response = await client.get(
+                    "https://api.pagerduty.com/incidents",
+                    headers=headers,
+                    params={
+                        "date_range": "all",
+                        "statuses[]": "resolved",
+                        "limit": limit,
+                        "offset": offset,
+                    },
+                )
+                data = response.json()
+                incidents = data["incidents"]
+                resolved_incidents.extend(incidents)
+                if not data["more"]:
+                    break
+                offset += limit
+        return resolved_incidents
+
+    async def load_data(self) -> List[Document]:
+        incidents = await self.get_incidents()
+
+        documents = []
+
+        for incident in incidents:
+            service = incident.get("service", {})
+            service_name = service.get("summary", "Unknown")
+
+            text = INCIDENT_TEXT_TEMPLATE.format(
+                title=incident["title"],
+                description=incident["description"],
+                summary=incident["summary"],
+                status=incident["status"],
+                service_name=service_name,
+                created_at=incident["created_at"],
+            )
+            metadata = {
+                "source": "PagerDuty",
+                "title": incident["title"],
+                "id": incident["id"],
+                "link": incident["html_url"],
+                "status": incident["status"],
+                "urgency": incident["urgency"],
+                "service_id": service.get("id", "Unknown"),
+                "first_trigger_log_entry_id": incident.get(
+                    "first_trigger_log_entry", {}
+                ).get("id", "Unknown"),
+                "created_at": incident["created_at"],
+                "updated_at": incident["updated_at"],
+            }
+
+            document = Document(doc_id=incident["id"], text=text, metadata=metadata)
+            documents.append(document)
+
+        return documents
diff --git a/services/data-processor/src/loaders/raw_readers/slack.py b/services/data-processor/src/loaders/readers/slack.py
similarity index 77%
rename from services/data-processor/src/loaders/raw_readers/slack.py
rename to services/data-processor/src/loaders/readers/slack.py
index dded214..03f7d7b 100644
--- a/services/data-processor/src/loaders/raw_readers/slack.py
+++ b/services/data-processor/src/loaders/readers/slack.py
@@ -103,8 +103,11 @@ class SlackReader(BasePydanticReader):
 
         """Read a message."""
 
+        # TODO: this method reads all the thread messages and creates one document
+        # At the moment, we don't use the usernames + timestamps. This can be a nice improvement.
         messages_text: List[str] = []
         next_cursor = None
+        most_recent_update = None
         while True:
             try:
                 # https://slack.com/api/conversations.replies
@@ -128,6 +131,18 @@ class SlackReader(BasePydanticReader):
                         **conversations_replies_kwargs  # type: ignore
                     )
                 messages = result["messages"]
+
+                for message in messages:
+                    last_edited = float(
+                        message.get("edited", {}).get("ts", message["ts"])
+                    )
+                    last_edited_utc = datetime.utcfromtimestamp(last_edited)
+                    if (
+                        most_recent_update is None
+                        or last_edited_utc > most_recent_update
+                    ):
+                        most_recent_update = last_edited_utc
+
                 messages_text.extend(message["text"] for message in messages)
                 if not result["has_more"]:
                     break
@@ -143,7 +158,10 @@ class SlackReader(BasePydanticReader):
                     time.sleep(int(e.response.headers["retry-after"]))
                 else:
                     logger.error(f"Error parsing conversation replies: {e}")
-        return "\n\n".join(messages_text)
+
+        most_recent_update = most_recent_update.isoformat(timespec="milliseconds") + "Z"
+
+        return ("\n\n".join(messages_text), most_recent_update)
 
     def _read_channel(self, channel_id: str, reverse_chronological: bool) -> str:
         from slack_sdk.errors import SlackApiError
@@ -162,6 +180,7 @@ class SlackReader(BasePydanticReader):
                     "channel": channel_id,
                     "cursor": next_cursor,
                     "latest": str(self.latest_date_timestamp),
+                    "include_all_metadata": True,
                 }
                 if self.earliest_date_timestamp is not None:
                     conversations_history_kwargs["oldest"] = str(
@@ -175,18 +194,34 @@ class SlackReader(BasePydanticReader):
                 logger.info(
                     f"{len(conversation_history)} messages found in {channel_id}"
                 )
-                result_messages.extend(
-                    (
-                        {
-                            **message,
-                            "text": self._read_message(channel_id, message["ts"]),
-                        }
-                        if message.get("thread_ts")
-                        == message["ts"]  # Message is a parent message of a thread
-                        else message
-                    )
-                    for message in tqdm(conversation_history, desc="Reading messages")
-                )
+
+                for message in tqdm(conversation_history, desc="Reading messages"):
+                    if message.get("thread_ts") == message["ts"]:
+                        # Message is a thread parent message. Let's explore this thread!
+                        text, most_recent_update = self._read_message(
+                            channel_id, message["ts"]
+                        )
+                        result_messages.append(
+                            {
+                                **message,
+                                "text": text,
+                                "updated_at": most_recent_update,
+                            }
+                        )
+                    else:
+                        last_edited = float(
+                            message.get("edited", {}).get("ts", message["ts"])
+                        )
+                        result_messages.append(
+                            {
+                                **message,
+                                "updated_at": datetime.utcfromtimestamp(
+                                    last_edited
+                                ).isoformat(timespec="milliseconds")
+                                + "Z",
+                            }
+                        )
+
                 if not result["has_more"]:
                     break
                 next_cursor = result["response_metadata"]["next_cursor"]
@@ -222,11 +257,20 @@ class SlackReader(BasePydanticReader):
             )
             # Remove messages with empty text
             messages = [message for message in messages if message["text"] != ""]
+            # debugging step
+            for message in messages:
+                if "glorious poop" in message["text"]:
+                    print("this is it boys")
+
             documents = [
                 Document(
                     id_=message["ts"],
                     text=message["text"],
-                    metadata={"channel_id": channel_id, "ts": message["ts"]},
+                    metadata={
+                        "channel_id": channel_id,
+                        "ts": message["ts"],
+                        "updated_at": message["updated_at"],
+                    },
                 )
                 for message in messages
             ]
diff --git a/services/data-processor/src/loaders/slack.py b/services/data-processor/src/loaders/slack.py
index 310aa86..b5b4b8c 100644
--- a/services/data-processor/src/loaders/slack.py
+++ b/services/data-processor/src/loaders/slack.py
@@ -6,7 +6,7 @@ from slack_sdk import WebClient
 
 from typing import List
 
-from loaders.raw_readers.slack import SlackReader
+from loaders.readers.slack import SlackReader
 
 
 def join_channels(client: WebClient, channel_ids: List[str]):
@@ -27,7 +27,6 @@ def fetch_slack_documents(integration: Integration):
         types=["public_channel", "private_channel"],
     )
     channel_ids = [channel["id"] for channel in channels["channels"]]
-
     id2name = {channel["id"]: channel["name"] for channel in channels["channels"]}
 
     # Try to join the channels, to avoid "not_in_channel" in Slack.
diff --git a/services/data-processor/src/loaders/utils/github_client.py b/services/data-processor/src/loaders/utils/github_client.py
new file mode 100644
index 0000000..16a7d7a
--- /dev/null
+++ b/services/data-processor/src/loaders/utils/github_client.py
@@ -0,0 +1,611 @@
+"""
+Github API client for the GPT-Index library.
+
+This module contains the Github API client for the GPT-Index library.
+It is used by the Github readers to retrieve the data from Github.
+"""
+
+import os
+
+from dataclasses import dataclass, field
+from typing import Any, Dict, List, Optional, Protocol
+
+from dataclasses_json import DataClassJsonMixin, config
+from httpx import HTTPError
+
+
+@dataclass
+class GitTreeResponseModel(DataClassJsonMixin):
+    """
+    Dataclass for the response from the Github API's getTree endpoint.
+
+    Attributes:
+        - sha (str): SHA1 checksum ID of the tree.
+        - url (str): URL for the tree.
+        - tree (List[GitTreeObject]): List of objects in the tree.
+        - truncated (bool): Whether the tree is truncated.
+
+    Examples:
+        >>> tree = client.get_tree("owner", "repo", "branch")
+        >>> tree.sha
+    """
+
+    @dataclass
+    class GitTreeObject(DataClassJsonMixin):
+        """
+        Dataclass for the objects in the tree.
+
+        Attributes:
+            - path (str): Path to the object.
+            - mode (str): Mode of the object.
+            - type (str): Type of the object.
+            - sha (str): SHA1 checksum ID of the object.
+            - url (str): URL for the object.
+            - size (Optional[int]): Size of the object (only for blobs).
+        """
+
+        path: str
+        mode: str
+        type: str
+        sha: str
+        url: str
+        size: Optional[int] = None
+
+    sha: str
+    url: str
+    tree: List[GitTreeObject]
+    truncated: bool
+
+
+@dataclass
+class GitBlobResponseModel(DataClassJsonMixin):
+    """
+    Dataclass for the response from the Github API's getBlob endpoint.
+
+    Attributes:
+        - content (str): Content of the blob.
+        - encoding (str): Encoding of the blob.
+        - url (str): URL for the blob.
+        - sha (str): SHA1 checksum ID of the blob.
+        - size (int): Size of the blob.
+        - node_id (str): Node ID of the blob.
+    """
+
+    content: str
+    encoding: str
+    url: str
+    sha: str
+    size: int
+    node_id: str
+
+
+@dataclass
+class GitCommitResponseModel(DataClassJsonMixin):
+    """
+    Dataclass for the response from the Github API's getCommit endpoint.
+
+    Attributes:
+        - tree (Tree): Tree object for the commit.
+    """
+
+    @dataclass
+    class Commit(DataClassJsonMixin):
+        """Dataclass for the commit object in the commit. (commit.commit)."""
+
+        @dataclass
+        class Tree(DataClassJsonMixin):
+            """
+            Dataclass for the tree object in the commit.
+
+            Attributes:
+                - sha (str): SHA for the commit
+            """
+
+            sha: str
+
+        @dataclass
+        class Author(DataClassJsonMixin):
+            """
+            Dataclass for the author object in the commit.
+
+            Attributes:
+                - name (str): Name of the author
+                - email (str): Email of the author
+                - date (str): Date of the commit
+            """
+
+            name: str
+            email: str
+            date: str
+
+        tree: Tree
+        author: Author
+
+    commit: Commit
+    url: str
+    sha: str
+
+
+@dataclass
+class GitCommitItem(DataClassJsonMixin):
+    """Dataclass for a single commit in the commits list."""
+
+    sha: str
+    node_id: str
+    commit: GitCommitResponseModel.Commit
+    url: str
+    html_url: str
+    comments_url: str
+    author: Optional[Dict[str, Any]]
+    committer: Optional[Dict[str, Any]]
+    parents: List[Dict[str, str]]
+
+
+GitCommitsResponseModel = List[GitCommitItem]
+
+
+@dataclass
+class GitBranchResponseModel(DataClassJsonMixin):
+    """
+    Dataclass for the response from the Github API's getBranch endpoint.
+
+    Attributes:
+        - commit (Commit): Commit object for the branch.
+    """
+
+    @dataclass
+    class Commit(DataClassJsonMixin):
+        """Dataclass for the commit object in the branch. (commit.commit)."""
+
+        @dataclass
+        class Commit(DataClassJsonMixin):
+            """Dataclass for the commit object in the commit. (commit.commit.tree)."""
+
+            @dataclass
+            class Tree(DataClassJsonMixin):
+                """
+                Dataclass for the tree object in the commit.
+
+                Usage: commit.commit.tree.sha
+                """
+
+                sha: str
+
+            tree: Tree
+
+        commit: Commit
+
+    @dataclass
+    class Links(DataClassJsonMixin):
+        _self: str = field(metadata=config(field_name="self"))
+        html: str
+
+    commit: Commit
+    name: str
+    _links: Links
+
+
+class BaseGithubClient(Protocol):
+    def get_all_endpoints(self) -> Dict[str, str]: ...
+
+    async def request(
+        self,
+        endpoint: str,
+        method: str,
+        headers: Dict[str, Any] = {},
+        **kwargs: Any,
+    ) -> Any: ...
+
+    async def get_tree(
+        self,
+        owner: str,
+        repo: str,
+        tree_sha: str,
+    ) -> GitTreeResponseModel: ...
+
+    async def get_blob(
+        self,
+        owner: str,
+        repo: str,
+        file_sha: str,
+    ) -> Optional[GitBlobResponseModel]: ...
+
+    async def get_commit(
+        self,
+        owner: str,
+        repo: str,
+        commit_sha: str,
+    ) -> GitCommitResponseModel: ...
+
+    async def get_branch(
+        self,
+        owner: str,
+        repo: str,
+        branch: Optional[str],
+        branch_name: Optional[str],
+    ) -> GitBranchResponseModel: ...
+
+
+class GithubClient:
+    """
+    An asynchronous client for interacting with the Github API.
+
+    This client is used for making API requests to Github.
+    It provides methods for accessing the Github API endpoints.
+    The client requires a Github token for authentication,
+    which can be passed as an argument or set as an environment variable.
+    If no Github token is provided, the client will raise a ValueError.
+
+    Examples:
+        >>> client = GithubClient("my_github_token")
+        >>> branch_info = client.get_branch("owner", "repo", "branch")
+    """
+
+    DEFAULT_BASE_URL = "https://api.github.com"
+    DEFAULT_API_VERSION = "2022-11-28"
+
+    def __init__(
+        self,
+        github_token: Optional[str] = None,
+        base_url: str = DEFAULT_BASE_URL,
+        api_version: str = DEFAULT_API_VERSION,
+        verbose: bool = False,
+        fail_on_http_error: bool = True,
+    ) -> None:
+        """
+        Initialize the GithubClient.
+
+        Args:
+            - github_token (str): Github token for authentication.
+                If not provided, the client will try to get it from
+                the GITHUB_TOKEN environment variable.
+            - base_url (str): Base URL for the Github API
+                (defaults to "https://api.github.com").
+            - api_version (str): Github API version (defaults to "2022-11-28").
+            - verbose (bool): Whether to print verbose output (defaults to False).
+            - fail_on_http_error (bool): Whether to raise an exception on HTTP errors (defaults to True).
+
+        Raises:
+            ValueError: If no Github token is provided.
+        """
+        if github_token is None:
+            github_token = os.getenv("GITHUB_TOKEN")
+            if github_token is None:
+                raise ValueError(
+                    "Please provide a Github token. "
+                    + "You can do so by passing it as an argument to the GithubReader,"
+                    + "or by setting the GITHUB_TOKEN environment variable."
+                )
+
+        self._base_url = base_url
+        self._api_version = api_version
+        self._verbose = verbose
+        self._fail_on_http_error = fail_on_http_error
+
+        self._endpoints = {
+            "getTree": "/repos/{owner}/{repo}/git/trees/{tree_sha}",
+            "getBranch": "/repos/{owner}/{repo}/branches/{branch}",
+            "getBlob": "/repos/{owner}/{repo}/git/blobs/{file_sha}",
+            "getCommit": "/repos/{owner}/{repo}/commits/{commit_sha}",
+            "getCommits": "/repos/{owner}/{repo}/commits",
+        }
+
+        self._headers = {
+            "Accept": "application/vnd.github+json",
+            "Authorization": f"Bearer {github_token}",
+            "X-GitHub-Api-Version": f"{self._api_version}",
+        }
+
+    def get_all_endpoints(self) -> Dict[str, str]:
+        """Get all available endpoints."""
+        return {**self._endpoints}
+
+    async def request(
+        self,
+        endpoint: str,
+        method: str,
+        headers: Dict[str, Any] = {},
+        timeout: Optional[int] = 5,
+        retries: int = 0,
+        **kwargs: Any,
+    ) -> Any:
+        """
+        Make an API request to the Github API.
+
+        This method is used for making API requests to the Github API.
+        It is used internally by the other methods in the client.
+
+        Args:
+            - `endpoint (str)`: Name of the endpoint to make the request to.
+            - `method (str)`: HTTP method to use for the request.
+            - `headers (dict)`: HTTP headers to include in the request.
+            - `timeout (int or None)`: Timeout for the request in seconds. Default is 5.
+            - `retries (int)`: Number of retries for the request. Default is 0.
+            - `**kwargs`: Keyword arguments to pass to the endpoint URL.
+
+        Returns:
+            - `response (httpx.Response)`: Response from the API request.
+
+        Raises:
+            - ImportError: If the `httpx` library is not installed.
+            - httpx.HTTPError: If the API request fails and fail_on_http_error is True.
+
+        Examples:
+            >>> response = client.request("getTree", "GET",
+                                owner="owner", repo="repo",
+                                tree_sha="tree_sha", timeout=5, retries=0)
+        """
+        try:
+            import httpx
+        except ImportError:
+            raise ImportError(
+                "Please install httpx to use the GithubRepositoryReader. "
+                "You can do so by running `pip install httpx`."
+            )
+
+        _headers = {**self._headers, **headers}
+
+        _client: httpx.AsyncClient
+        async with httpx.AsyncClient(
+            headers=_headers,
+            base_url=self._base_url,
+            timeout=timeout,
+            transport=httpx.AsyncHTTPTransport(retries=retries),
+        ) as _client:
+            try:
+                response = await _client.request(
+                    method, url=self._endpoints[endpoint].format(**kwargs)
+                )
+            except httpx.HTTPError as excp:
+                print(f"HTTP Exception for {excp.request.url} - {excp}")
+                raise excp  # noqa: TRY201
+            return response
+
+    async def get_branch(
+        self,
+        owner: str,
+        repo: str,
+        branch: Optional[str] = None,
+        branch_name: Optional[str] = None,
+        timeout: Optional[int] = 5,
+        retries: int = 0,
+    ) -> GitBranchResponseModel:
+        """
+        Get information about a branch. (Github API endpoint: getBranch).
+
+        Args:
+            - `owner (str)`: Owner of the repository.
+            - `repo (str)`: Name of the repository.
+            - `branch (str)`: Name of the branch.
+            - `branch_name (str)`: Name of the branch (alternative to `branch`).
+            - `timeout (int or None)`: Timeout for the request in seconds. Default is 5.
+            - `retries (int)`: Number of retries for the request. Default is 0.
+
+        Returns:
+            - `branch_info (GitBranchResponseModel)`: Information about the branch.
+
+        Examples:
+            >>> branch_info = client.get_branch("owner", "repo", "branch")
+        """
+        if branch is None:
+            if branch_name is None:
+                raise ValueError("Either branch or branch_name must be provided.")
+            branch = branch_name
+
+        return GitBranchResponseModel.from_json(
+            (
+                await self.request(
+                    "getBranch",
+                    "GET",
+                    owner=owner,
+                    repo=repo,
+                    branch=branch,
+                    timeout=timeout,
+                    retries=retries,
+                )
+            ).text
+        )
+
+    async def get_tree(
+        self,
+        owner: str,
+        repo: str,
+        tree_sha: str,
+        timeout: Optional[int] = 5,
+        retries: int = 0,
+    ) -> GitTreeResponseModel:
+        """
+        Get information about a tree. (Github API endpoint: getTree).
+
+        Args:
+            - `owner (str)`: Owner of the repository.
+            - `repo (str)`: Name of the repository.
+            - `tree_sha (str)`: SHA of the tree.
+            - `timeout (int or None)`: Timeout for the request in seconds. Default is 5.
+            - `retries (int)`: Number of retries for the request. Default is 0.
+
+        Returns:
+            - `tree_info (GitTreeResponseModel)`: Information about the tree.
+
+        Examples:
+            >>> tree_info = client.get_tree("owner", "repo", "tree_sha")
+        """
+        response = (
+            await self.request(
+                "getTree",
+                "GET",
+                owner=owner,
+                repo=repo,
+                tree_sha=tree_sha,
+                timeout=timeout,
+                retries=retries,
+            )
+        ).text
+        return GitTreeResponseModel.from_json(response)
+
+    async def get_blob(
+        self,
+        owner: str,
+        repo: str,
+        file_sha: str,
+        timeout: Optional[int] = 5,
+        retries: int = 0,
+    ) -> Optional[GitBlobResponseModel]:
+        """
+        Get information about a blob. (Github API endpoint: getBlob).
+
+        Args:
+            - `owner (str)`: Owner of the repository.
+            - `repo (str)`: Name of the repository.
+            - `file_sha (str)`: SHA of the file.
+            - `timeout (int or None)`: Timeout for the request in seconds. Default is 5.
+            - `retries (int)`: Number of retries for the request. Default is 0.
+
+        Returns:
+            - `blob_info (GitBlobResponseModel)`: Information about the blob.
+
+        Examples:
+            >>> blob_info = client.get_blob("owner", "repo", "file_sha")
+        """
+        try:
+            return GitBlobResponseModel.from_json(
+                (
+                    await self.request(
+                        "getBlob",
+                        "GET",
+                        owner=owner,
+                        repo=repo,
+                        file_sha=file_sha,
+                        timeout=timeout,
+                        retries=retries,
+                    )
+                ).text
+            )
+        except KeyError:
+            print(f"Failed to get blob for {owner}/{repo}/{file_sha}")
+            return None
+        except HTTPError as excp:
+            print(f"HTTP Exception for {excp.request.url} - {excp}")
+            if self._fail_on_http_error:
+                raise
+            else:
+                return None
+
+    async def get_commit(
+        self,
+        owner: str,
+        repo: str,
+        commit_sha: str,
+        timeout: Optional[int] = 5,
+        retries: int = 0,
+    ) -> GitCommitResponseModel:
+        """
+        Get information about a commit. (Github API endpoint: getCommit).
+
+        Args:
+            - `owner (str)`: Owner of the repository.
+            - `repo (str)`: Name of the repository.
+            - `commit_sha (str)`: SHA of the commit.
+            - `timeout (int or None)`: Timeout for the request in seconds. Default is 5.
+            - `retries (int)`: Number of retries for the request. Default is 0.
+
+        Returns:
+            - `commit_info (GitCommitResponseModel)`: Information about the commit.
+
+        Examples:
+            >>> commit_info = client.get_commit("owner", "repo", "commit_sha")
+        """
+        return GitCommitResponseModel.from_json(
+            (
+                await self.request(
+                    "getCommit",
+                    "GET",
+                    owner=owner,
+                    repo=repo,
+                    commit_sha=commit_sha,
+                    timeout=timeout,
+                    retries=retries,
+                )
+            ).text
+        )
+
+    async def get_commits(
+        self,
+        owner: str,
+        repo: str,
+        sha: Optional[str] = None,
+        path: Optional[str] = None,
+        author: Optional[str] = None,
+        since: Optional[str] = None,
+        until: Optional[str] = None,
+        per_page: Optional[int] = None,
+        page: Optional[int] = None,
+        timeout: Optional[int] = 5,
+        retries: int = 0,
+    ) -> GitCommitsResponseModel:
+        """
+        Get commits for a repository. (Github API endpoint: getCommits).
+
+        Args:
+            - owner (str): Owner of the repository.
+            - repo (str): Name of the repository.
+            - sha (str, optional): SHA or branch to start listing commits from.
+            - path (str, optional): Only commits containing this file path will be returned.
+            - author (str, optional): GitHub login or email address by which to filter by commit author.
+            - since (str, optional): Only commits after this date will be returned. ISO 8601 format: YYYY-MM-DDTHH:MM:SSZ.
+            - until (str, optional): Only commits before this date will be returned. ISO 8601 format: YYYY-MM-DDTHH:MM:SSZ.
+            - per_page (int, optional): Results per page (max 100).
+            - page (int, optional): Page number of the results to fetch.
+            - timeout (int or None): Timeout for the request in seconds. Default is 5.
+            - retries (int): Number of retries for the request. Default is 0.
+
+        Returns:
+            - commits_info (GitCommitsResponseModel): Information about the commits.
+
+        Examples:
+            >>> commits_info = client.get_commits("owner", "repo", sha="main")
+        """
+        params = {
+            "sha": sha,
+            "path": path,
+            "author": author,
+            "since": since,
+            "until": until,
+            "per_page": per_page,
+            "page": page,
+        }
+        params = {k: v for k, v in params.items() if v is not None}
+
+        response = await self.request(
+            "getCommits",
+            "GET",
+            owner=owner,
+            repo=repo,
+            params=params,
+            timeout=timeout,
+            retries=retries,
+        )
+        data = response.json()
+        return [GitCommitItem.from_dict(item) for item in data]
+
+
+if __name__ == "__main__":
+    import asyncio
+
+    async def main() -> None:
+        """Test the GithubClient."""
+        client = GithubClient()
+        response = await client.get_tree(
+            owner="ahmetkca", repo="CommitAI", tree_sha="with-body"
+        )
+
+        for obj in response.tree:
+            if obj.type == "blob":
+                print(obj.path)
+                print(obj.sha)
+                blob_response = await client.get_blob(
+                    owner="ahmetkca", repo="CommitAI", file_sha=obj.sha
+                )
+                print(blob_response.content if blob_response else "None")
+
+    asyncio.run(main())
diff --git a/services/data-processor/src/main.py b/services/data-processor/src/main.py
index 96cbe1b..8e65387 100644
--- a/services/data-processor/src/main.py
+++ b/services/data-processor/src/main.py
@@ -49,11 +49,9 @@ async def start_build_index(
 
     # TODO: we re-create the index every time. We need to consider
     # changing this in the future
-    existing_index = await get_index_by_organization_id(organization_id)
-    if existing_index:
-        await delete_index_by_id(existing_index["_id"])
-
-    index = await create_index(organization_id, data_sources, "chromadb")
+    index = await get_index_by_organization_id(organization_id)
+    if not index:
+        index = await create_index(organization_id, data_sources, "chromadb")
 
     background_tasks.add_task(
         build_index,
diff --git a/services/data-processor/src/rag/chromadb.py b/services/data-processor/src/rag/chromadb.py
index 775364a..2d9cbf6 100644
--- a/services/data-processor/src/rag/chromadb.py
+++ b/services/data-processor/src/rag/chromadb.py
@@ -1,6 +1,6 @@
 import chromadb
 from chromadb.config import Settings
-from llama_index.vector_stores.chroma import (
+from .raw_vector_stores.chromadb import (
     ChromaVectorStore as LIChromaVectorStore,
 )
 from rag.base import BaseVectorStore
diff --git a/services/data-processor/src/rag/raw_vector_stores/chromadb.py b/services/data-processor/src/rag/raw_vector_stores/chromadb.py
new file mode 100644
index 0000000..f3f613f
--- /dev/null
+++ b/services/data-processor/src/rag/raw_vector_stores/chromadb.py
@@ -0,0 +1,423 @@
+# This file was taken from llama-index and adjusted
+# Main adjustments
+# 1. Support for $in operator
+# 2. Support for deletion of nodes using multiple ref_doc_ids
+
+"""Chroma vector store."""
+
+import logging
+import math
+from typing import Any, Dict, Generator, List, Optional, cast
+
+import chromadb
+from chromadb.api.models.Collection import Collection
+from llama_index.core.bridge.pydantic import Field, PrivateAttr
+from llama_index.core.schema import BaseNode, MetadataMode, TextNode
+from llama_index.core.utils import truncate_text
+from llama_index.core.vector_stores.types import (
+    BasePydanticVectorStore,
+    MetadataFilters,
+    VectorStoreQuery,
+    VectorStoreQueryResult,
+)
+from llama_index.core.vector_stores.utils import (
+    legacy_metadata_dict_to_node,
+    metadata_dict_to_node,
+    node_to_metadata_dict,
+)
+
+logger = logging.getLogger(__name__)
+
+
+def _transform_chroma_filter_condition(condition: str) -> str:
+    """Translate standard metadata filter op to Chroma specific spec."""
+    if condition == "and":
+        return "$and"
+    elif condition == "or":
+        return "$or"
+    else:
+        raise ValueError(f"Filter condition {condition} not supported")
+
+
+def _transform_chroma_filter_operator(operator: str) -> str:
+    """Translate standard metadata filter operator to Chroma specific spec."""
+    if operator == "!=":
+        return "$ne"
+    elif operator == "==":
+        return "$eq"
+    elif operator == ">":
+        return "$gt"
+    elif operator == "<":
+        return "$lt"
+    elif operator == ">=":
+        return "$gte"
+    elif operator == "<=":
+        return "$lte"
+    elif operator == "in":
+        return "$in"
+    else:
+        raise ValueError(f"Filter operator {operator} not supported")
+
+
+def _to_chroma_filter(
+    standard_filters: MetadataFilters,
+) -> dict:
+    """Translate standard metadata filters to Chroma specific spec."""
+    filters = {}
+    filters_list = []
+    condition = standard_filters.condition or "and"
+    condition = _transform_chroma_filter_condition(condition)
+    if standard_filters.filters:
+        for filter in standard_filters.filters:
+            if filter.operator:
+                filters_list.append(
+                    {
+                        filter.key: {
+                            _transform_chroma_filter_operator(
+                                filter.operator
+                            ): filter.value
+                        }
+                    }
+                )
+            else:
+                filters_list.append({filter.key: filter.value})
+
+    if len(filters_list) == 1:
+        # If there is only one filter, return it directly
+        return filters_list[0]
+    elif len(filters_list) > 1:
+        filters[condition] = filters_list
+    return filters
+
+
+import_err_msg = "`chromadb` package not found, please run `pip install chromadb`"
+
+MAX_CHUNK_SIZE = 41665  # One less than the max chunk size for ChromaDB
+
+
+def chunk_list(
+    lst: List[BaseNode], max_chunk_size: int
+) -> Generator[List[BaseNode], None, None]:
+    """Yield successive max_chunk_size-sized chunks from lst.
+
+    Args:
+        lst (List[BaseNode]): list of nodes with embeddings
+        max_chunk_size (int): max chunk size
+
+    Yields:
+        Generator[List[BaseNode], None, None]: list of nodes with embeddings
+    """
+    for i in range(0, len(lst), max_chunk_size):
+        yield lst[i : i + max_chunk_size]
+
+
+class ChromaVectorStore(BasePydanticVectorStore):
+    """Chroma vector store.
+
+    In this vector store, embeddings are stored within a ChromaDB collection.
+
+    During query time, the index uses ChromaDB to query for the top
+    k most similar nodes.
+
+    Args:
+        chroma_collection (chromadb.api.models.Collection.Collection):
+            ChromaDB collection instance
+
+    Examples:
+        `pip install llama-index-vector-stores-chroma`
+
+        ```python
+        import chromadb
+        from llama_index.vector_stores.chroma import ChromaVectorStore
+
+        # Create a Chroma client and collection
+        chroma_client = chromadb.EphemeralClient()
+        chroma_collection = chroma_client.create_collection("example_collection")
+
+        # Set up the ChromaVectorStore and StorageContext
+        vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
+        ```
+
+    """
+
+    stores_text: bool = True
+    flat_metadata: bool = True
+
+    collection_name: Optional[str]
+    host: Optional[str]
+    port: Optional[str]
+    ssl: bool
+    headers: Optional[Dict[str, str]]
+    persist_dir: Optional[str]
+    collection_kwargs: Dict[str, Any] = Field(default_factory=dict)
+
+    _collection: Any = PrivateAttr()
+
+    def __init__(
+        self,
+        chroma_collection: Optional[Any] = None,
+        collection_name: Optional[str] = None,
+        host: Optional[str] = None,
+        port: Optional[str] = None,
+        ssl: bool = False,
+        headers: Optional[Dict[str, str]] = None,
+        persist_dir: Optional[str] = None,
+        collection_kwargs: Optional[dict] = None,
+        **kwargs: Any,
+    ) -> None:
+        """Init params."""
+        collection_kwargs = collection_kwargs or {}
+        if chroma_collection is None:
+            client = chromadb.HttpClient(host=host, port=port, ssl=ssl, headers=headers)
+            self._collection = client.get_or_create_collection(
+                name=collection_name, **collection_kwargs
+            )
+        else:
+            self._collection = cast(Collection, chroma_collection)
+
+        super().__init__(
+            host=host,
+            port=port,
+            ssl=ssl,
+            headers=headers,
+            collection_name=collection_name,
+            persist_dir=persist_dir,
+            collection_kwargs=collection_kwargs or {},
+        )
+
+    @classmethod
+    def from_collection(cls, collection: Any) -> "ChromaVectorStore":
+        try:
+            from chromadb import Collection
+        except ImportError:
+            raise ImportError(import_err_msg)
+
+        if not isinstance(collection, Collection):
+            raise Exception("argument is not chromadb collection instance")
+
+        return cls(chroma_collection=collection)
+
+    @classmethod
+    def from_params(
+        cls,
+        collection_name: str,
+        host: Optional[str] = None,
+        port: Optional[str] = None,
+        ssl: bool = False,
+        headers: Optional[Dict[str, str]] = None,
+        persist_dir: Optional[str] = None,
+        collection_kwargs: dict = {},
+        **kwargs: Any,
+    ) -> "ChromaVectorStore":
+        if persist_dir:
+            client = chromadb.PersistentClient(path=persist_dir)
+            collection = client.get_or_create_collection(
+                name=collection_name, **collection_kwargs
+            )
+        elif host and port:
+            client = chromadb.HttpClient(host=host, port=port, ssl=ssl, headers=headers)
+            collection = client.get_or_create_collection(
+                name=collection_name, **collection_kwargs
+            )
+        else:
+            raise ValueError(
+                "Either `persist_dir` or (`host`,`port`) must be specified"
+            )
+        return cls(
+            chroma_collection=collection,
+            host=host,
+            port=port,
+            ssl=ssl,
+            headers=headers,
+            persist_dir=persist_dir,
+            collection_kwargs=collection_kwargs,
+            **kwargs,
+        )
+
+    @classmethod
+    def class_name(cls) -> str:
+        return "ChromaVectorStore"
+
+    def add(self, nodes: List[BaseNode], **add_kwargs: Any) -> List[str]:
+        """Add nodes to index.
+
+        Args:
+            nodes: List[BaseNode]: list of nodes with embeddings
+
+        """
+        if not self._collection:
+            raise ValueError("Collection not initialized")
+
+        max_chunk_size = MAX_CHUNK_SIZE
+        node_chunks = chunk_list(nodes, max_chunk_size)
+
+        all_ids = []
+        for node_chunk in node_chunks:
+            embeddings = []
+            metadatas = []
+            ids = []
+            documents = []
+            for node in node_chunk:
+                embeddings.append(node.get_embedding())
+                metadata_dict = node_to_metadata_dict(
+                    node, remove_text=True, flat_metadata=self.flat_metadata
+                )
+                for key in metadata_dict:
+                    if metadata_dict[key] is None:
+                        metadata_dict[key] = ""
+                metadatas.append(metadata_dict)
+                ids.append(node.node_id)
+                documents.append(node.get_content(metadata_mode=MetadataMode.NONE))
+
+            self._collection.add(
+                embeddings=embeddings,
+                ids=ids,
+                metadatas=metadatas,
+                documents=documents,
+            )
+            all_ids.extend(ids)
+
+        return all_ids
+
+    def delete(self, ref_doc_id: str | List[str], **delete_kwargs: Any) -> None:
+        """
+        Delete nodes using with ref_doc_id.
+
+        Args:
+            ref_doc_id (str): The doc_id of the document to delete.
+
+        """
+        if isinstance(ref_doc_id, list):
+            self._collection.delete(where={"document_id": {"$in": ref_doc_id}})
+        else:
+            self._collection.delete(where={"document_id": ref_doc_id})
+
+    @property
+    def client(self) -> Any:
+        """Return client."""
+        return self._collection
+
+    def query(self, query: VectorStoreQuery, **kwargs: Any) -> VectorStoreQueryResult:
+        """Query index for top k most similar nodes.
+
+        Args:
+            query_embedding (List[float]): query embedding
+            similarity_top_k (int): top k most similar nodes
+
+        """
+        if query.filters is not None:
+            if "where" in kwargs:
+                raise ValueError(
+                    "Cannot specify metadata filters via both query and kwargs. "
+                    "Use kwargs only for chroma specific items that are "
+                    "not supported via the generic query interface."
+                )
+            where = _to_chroma_filter(query.filters)
+        else:
+            where = kwargs.pop("where", {})
+
+        if not query.query_embedding:
+            return self._get(limit=query.similarity_top_k, where=where, **kwargs)
+
+        return self._query(
+            query_embeddings=query.query_embedding,
+            n_results=query.similarity_top_k,
+            where=where,
+            **kwargs,
+        )
+
+    def _query(
+        self, query_embeddings: List["float"], n_results: int, where: dict, **kwargs
+    ) -> VectorStoreQueryResult:
+        results = self._collection.query(
+            query_embeddings=query_embeddings,
+            n_results=n_results,
+            where=where,
+            **kwargs,
+        )
+
+        logger.debug(f"> Top {len(results['documents'][0])} nodes:")
+        nodes = []
+        similarities = []
+        ids = []
+        for node_id, text, metadata, distance in zip(
+            results["ids"][0],
+            results["documents"][0],
+            results["metadatas"][0],
+            results["distances"][0],
+        ):
+            try:
+                node = metadata_dict_to_node(metadata)
+                node.set_content(text)
+            except Exception:
+                # NOTE: deprecated legacy logic for backward compatibility
+                metadata, node_info, relationships = legacy_metadata_dict_to_node(
+                    metadata
+                )
+
+                node = TextNode(
+                    text=text,
+                    id_=node_id,
+                    metadata=metadata,
+                    start_char_idx=node_info.get("start", None),
+                    end_char_idx=node_info.get("end", None),
+                    relationships=relationships,
+                )
+
+            nodes.append(node)
+
+            similarity_score = math.exp(-distance)
+            similarities.append(similarity_score)
+
+            logger.debug(
+                f"> [Node {node_id}] [Similarity score: {similarity_score}] "
+                f"{truncate_text(str(text), 100)}"
+            )
+            ids.append(node_id)
+
+        return VectorStoreQueryResult(nodes=nodes, similarities=similarities, ids=ids)
+
+    def _get(self, limit: int, where: dict, **kwargs) -> VectorStoreQueryResult:
+        results = self._collection.get(
+            limit=limit,
+            where=where,
+            **kwargs,
+        )
+
+        logger.debug(f"> Top {len(results['documents'])} nodes:")
+        nodes = []
+        ids = []
+
+        if not results["ids"]:
+            results["ids"] = [[]]
+
+        for node_id, text, metadata in zip(
+            results["ids"][0], results["documents"], results["metadatas"]
+        ):
+            try:
+                node = metadata_dict_to_node(metadata)
+                node.set_content(text)
+            except Exception:
+                # NOTE: deprecated legacy logic for backward compatibility
+                metadata, node_info, relationships = legacy_metadata_dict_to_node(
+                    metadata
+                )
+
+                node = TextNode(
+                    text=text,
+                    id_=node_id,
+                    metadata=metadata,
+                    start_char_idx=node_info.get("start", None),
+                    end_char_idx=node_info.get("end", None),
+                    relationships=relationships,
+                )
+
+            nodes.append(node)
+
+            logger.debug(
+                f"> [Node {node_id}] [Similarity score: N/A - using get()] "
+                f"{truncate_text(str(text), 100)}"
+            )
+            ids.append(node_id)
+
+        return VectorStoreQueryResult(nodes=nodes, ids=ids)
