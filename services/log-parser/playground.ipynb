{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "file_path = 'log_response.jsonl'\n",
    "jsonObj = pd.read_json(path_or_buf=file_path, lines=True)\n",
    "\n",
    "raw_logs = []\n",
    "for index, row in jsonObj.iterrows():\n",
    "    batch = row['result']\n",
    "\n",
    "    \n",
    "    for result in batch['results']:\n",
    "        raw_logs.append(result)\n",
    "\n",
    "df_raw_logs = pd.DataFrame(raw_logs)\n",
    "\n",
    "metadata_df = df_raw_logs['metadata'].apply(lambda row: {obj['key']: obj.get('value') for obj in row})\n",
    "labels_df = df_raw_logs['labels'].apply(lambda row: {obj['key']: obj.get('value') for obj in row})\n",
    "userData_df = df_raw_logs['userData'].apply(lambda row: pd.json_normalize(json.loads(row)).to_dict())\n",
    "\n",
    "# # Create DataFrames from the extracted dictionaries\n",
    "userData_df = pd.json_normalize(userData_df)\n",
    "metadata_df = pd.json_normalize(metadata_df)\n",
    "labels_df = pd.json_normalize(labels_df)\n",
    "\n",
    "# # Concatenate the new DataFrames with the original DataFrame\n",
    "df_logs = pd.concat([df_raw_logs.drop(['metadata', 'labels', 'userData'], axis=1),\n",
    "                           userData_df, metadata_df, labels_df], axis=1)\n",
    "\n",
    "df_logs.columns = df_logs.columns.str.replace('.0', '')\n",
    "df_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = []\n",
    "content_col = \"logRecord.body\"\n",
    "level_col = \"logRecord.severityText\"\n",
    "\n",
    "for index, row in df_logs.iterrows():\n",
    "    timestamp = row[\"timestamp\"]\n",
    "    content = row[content_col]\n",
    "    level = row[level_col]\n",
    "\n",
    "    log = f\"{timestamp} {level}: {content}\"\n",
    "    logs.append(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_logs['timestamp'] + \" \" + df_logs[level_col] + \": \" + df_logs[content_col]).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analyze import analyze_logs\n",
    "\n",
    "df_enriched_logs, df_templates = analyze_logs(logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_logs = df_enriched_logs.merge(df_logs, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_columns = [\"EventId\", \"EventTemplate\", \"ParameterList\"]\n",
    "included_columns = [col for col in df_final_logs.columns if col not in excluded_columns]\n",
    "\n",
    "def process_template_group(group): \n",
    "    total_values = {}\n",
    "\n",
    "    # Collect unique values from each column in the group\n",
    "    for col in included_columns:\n",
    "        try:\n",
    "            values = group[col].unique().tolist()\n",
    "            nvalues = len(values)\n",
    "            if nvalues == len(group):\n",
    "                continue\n",
    "            if len(values) > 10:\n",
    "                values = [f\"{nvalues} unique values (too many to display)\"]\n",
    "            concatenated_values = \", \".join(values)\n",
    "            if len(concatenated_values) > 50:\n",
    "                concatenated_values = concatenated_values[:50] + \"... (truncated)\"\n",
    "            total_values[col] = concatenated_values\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "    # Calculate occurrences of the group\n",
    "    total_values[\"occurrences\"] = len(group)\n",
    "    \n",
    "    series = pd.Series(total_values)\n",
    "    df = pd.DataFrame(series).T\n",
    "    return df\n",
    "\n",
    "df_enriched_templates = df_final_logs.groupby(\"EventTemplate\").apply(process_template_group)\n",
    "df_enriched_templates = df_enriched_templates.reset_index().drop(columns=[\"level_1\", \"Content\"])\n",
    "df_enriched_templates['percentage'] = df_enriched_templates['occurrences'] / len(df_final_logs) * 100\n",
    "df_enriched_templates = df_enriched_templates.sort_values(\"occurrences\", ascending=False)\n",
    "\n",
    "# Remove log groups with only one occurrence\n",
    "df_enriched_templates = df_enriched_templates[df_enriched_templates[\"occurrences\"] > 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "records = df_enriched_templates.to_dict(orient=\"records\")\n",
    "for record in records:\n",
    "    keys_to_remove = []\n",
    "    for key in record.keys():\n",
    "        is_none = record[key] is None\n",
    "        is_nan = isinstance(record[key], float) and math.isnan(record[key])\n",
    "\n",
    "        is_invalid = is_none or is_nan\n",
    "        if is_invalid:\n",
    "            keys_to_remove.append(key)\n",
    "        \n",
    "    for key in keys_to_remove:\n",
    "        del record[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"templates.json\", \"w\") as f:\n",
    "    json.dump(records, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "logparser-hmV3RqGv-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
